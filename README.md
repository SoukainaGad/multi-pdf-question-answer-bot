````markdown
# ğŸ“š Multi-PDF Question Answering Bot â€” Powered by Llama-3.3-70B (Groq)

An intelligent **RAG (Retrieval-Augmented Generation)** app that lets you **upload multiple PDFs** and ask natural-language questions across all of them.  
Built with **Streamlit**, **LangChain**, **Chroma**, and **Groqâ€™s Llama-3.3-70B** model.

---

## ğŸš€ Demo

> Upload multiple PDFs â†’ Process them â†’ Ask any question â†’ Get AI-powered answers instantly!

---

## ğŸ§  Features

- ğŸ“‚ Upload and analyze multiple PDFs at once  
- ğŸ§© Automatic text chunking and embedding  
- âš¡ Query all documents simultaneously using RAG  
- ğŸ§  Powered by **Llama-3.3-70B (Groq)** for fast, high-quality responses  
- ğŸ’¾ Local vector storage via **ChromaDB**

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Clone this Repository
```bash
git clone https://github.com/YOUR_USERNAME/multi-pdf-rag.git
cd multi-pdf-rag
````

### 2ï¸âƒ£ Create and Activate a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate      # macOS/Linux
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Environment Variables

Create a `.env` file in the root folder and add your Groq API key:

```bash
GROQ_API_KEY=your_groq_api_key_here
```

(You can copy `.env.example` and rename it to `.env`.)

---

## â–¶ï¸ Run the App

```bash
streamlit run app.py
```

Then open your browser at **[http://localhost:8501](http://localhost:8501)**

---

## ğŸ“ Project Structure

```
multi-pdf-rag/
â”œâ”€â”€ app.py                  # Streamlit user interface
â”œâ”€â”€ rag_utility.py          # Core RAG logic
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Template for environment variables
â”œâ”€â”€ README.md               # Documentation
â”œâ”€â”€ data/                   # Optional folder for uploaded PDFs
â””â”€â”€ multi_doc_vectorstore/  # ChromaDB persistence (auto-created)
```

---

## ğŸ§© How It Works

1. **Upload PDFs** â†’ Each document is read and parsed.
2. **Chunking & Embedding** â†’ Text is split into chunks and converted into embeddings.
3. **Storage** â†’ All embeddings are saved in a local **Chroma vector store**.
4. **Question Answering** â†’ When you ask a question, relevant chunks are retrieved and the **Llama-3.3-70B** model generates a context-aware answer.

---

## ğŸ§° Tech Stack

| Component       | Library / Service                   |
| --------------- | ----------------------------------- |
| Frontend UI     | Streamlit                           |
| Vector Database | Chroma                              |
| Embeddings      | Sentence Transformers (HuggingFace) |
| LLM             | Llama-3.3-70B (Groq)                |
| RAG Framework   | LangChain                           |
| PDF Parsing     | Unstructured                        |


